= Bench

== Running from Gradle

(from the root of the repo, values given are defaults)

TPC-H::
`./gradlew tpch -PscaleFactor=0.01`

AuctionMark::
`./gradlew auctionmark -Pduration=PT30S -Pthreads=8 -PscaleFactor=0.1`

Readings::
`./gradlew readings -PdeviceCount=10000 -PreadingCount=10000`

Products::
`./gradlew products`

IngestTsOverhead::
`./gradlew ingest -PdocCount=100000 -PbatchSizes=10,100,1000`


TSBS IoT::
`XTDB_LOGGING_LEVEL_BENCH_TSBS=debug ./gradlew :tsbs-iot -Pfile=/path/to/txs.transit.json`
+
For any reasonable scale you'll want to create a txs file beforehand - they take quite a while to generate in XT format.
See datasets -> `xtdb.tsbs`.

Add a YourKit snapshot with `-Pyourkit` - you should see a snapshot appear in your snapshot directory (default `~/Snapshots`) when the application shuts down.


In case you want to save the JSON benchmark log records to file you can append `-PbenchLogFile=tpch-0.01.log` to the command.

=== Running with custom config

You can run the benchmarks with a custom node config by passing `-PconfigFile path/to/config.yaml` to the gradle command.

`./gradlew auctionmark -PconfigFile=custom-config.yaml -Pduration=PT30S -Pthreads=8 -PscaleFactor=0.1`

=== Running with tracing enabled

You can enable distributed tracing by passing a tracing endpoint. This allows you to send traces to an OpenTelemetry-compatible backend (e.g., Grafana Tempo, Jaeger).

`./gradlew tpch -PtracingEndpoint=http://localhost:4318/v1/traces -PscaleFactor=0.01`

Note: The `--tracing-endpoint` flag is only used when no custom config file is provided. If you're using a custom config file, configure tracing in the YAML file instead (see the link:../../docs/src/content/docs/ops/config/monitoring.md[monitoring documentation]).

== XTDB Bench Docker Image

The xtdb-bench Docker image is automatically built and pushed to GitHub Container Registry (GHCR) against the main branch every night. (tagged with `nightly`)


You can also manually build and push the image from any branch using the workflow_dispatch trigger in the "Build xtdb-bench Docker Image" GitHub Actions workflow (tagged with the Git SHA).

=== Running the Docker Image

From the root of the repository, you can run a benchmark with:

[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly <benchmark-type> <opts...>
----

Replace <benchmark-type> with the name of the benchmark, and <opts...> with any additional CLI arguments (e.g., --node-dir, --config-file, etc.).

TPC-H::
[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly tpch \
  --scale-factor 0.01
----
AuctionMark::
[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly auctionmark \
  --duration PT30S \
  --scale-factor 0.1 \
  --threads 8
----
Readings::
[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly readings \
  --devices 10000
  --readings 10000
----
Products::
[source,bash]
----
# ensure you've either downloaded the products dataset
# from S3 to datasets/products.transit.msgpack.gz,
# or your process has permissions to do so

docker run --rm ghcr.io/xtdb/xtdb-bench:nightly products
----

==== Running with custom config

You can run the benchmarks with a custom node config by passing `--config-file path/to/config.yaml` to the docker command.

[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly auctionmark \
  --config-file custom-config.yaml \
  --duration PT30S \
  --threads 8 \
  --scale-factor 0.1
----

Included within the xtdb-bench are a number of config files for different cloud providers - see those under `cloud/config/`.

==== Running with tracing enabled

You can enable distributed tracing by passing the `--tracing-endpoint` flag. This allows you to send traces to an OpenTelemetry-compatible backend (e.g., Grafana Tempo, Jaeger).

[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly tpch \
  --tracing-endpoint http://tempo:4318/v1/traces \
  --scale-factor 0.01
----

Note: The `--tracing-endpoint` flag is only used when no custom config file is provided. If you're using a custom config file, configure tracing in the YAML file instead.

== Prometheus and Grafana

For local node monitoring, bring up a Prometheus and Grafana instance via `docker-compose up`.
By default the node serves Prometheus metrics under `localhost:8080/metrics` which Prometheus scrapes every 2 seconds.
If the metrics are not exposed under `localhost:8080` you need to tweak the `.config/prometheus.yaml` config accordingly.

Under `localhost:3001` you are able to access the Grafana UI (`admin` for user and password).
The XTDB node datasource should be setup, and there should be a dashboard available with some basic metrics.
If you tweak the dashboard or add a new one, save the changes in the corresponding file in `.config/dashboards/dashboard_name.json`.

== Running the benchmarks on the Cloud

=== Prerequisites

* AWS/Azure/GCP CLI
* Terraform
* Helm 3.0+

=== Authentication

Prior to running the benchmarks on the cloud, you need to set up your cloud provider credentials:

AWS::
[source,bash]
----
# Setup a profile named "xtdb-bench"
aws configure sso --profile xtdb-bench
# Logon to your profile:
aws sso login --profile xtdb-bench
----

Azure::
[source,bash]
----
# Login to Azure
az login --scope https://management.azure.com//.default
# Set the subscription
az account set --subscription "XTDB long-run reliability"
----

Google Cloud::
[source,bash]
----
# Login to GCP
gcloud auth login
# Set the project
gcloud config set project xtdb-scratch
----

=== Setup the Infra

We can setup the cloud infra using the "setup-infra.sh" script:

[source,bash]
----
./cloud/setup-infra.sh <aws|azure|google-cloud>
----

This will create the necessary resources for running the benchmarks on the cloud, and log you into the created Kubernetes cluster.

=== Run a benchmark

==== Basic Usage

Run benchmarks using the `run-bench.sh` script:

[source,bash]
----
./cloud/run-bench.sh <aws|azure|google-cloud> <benchmark-type> [helm-args...] [--no-cleanup]
----

==== Benchmark Types

TPC-H::
[source,bash]
----
./cloud/run-bench.sh azure tpch \
  --set tpch.scaleFactor=0.01
----

Readings::
[source,bash]
----
./cloud/run-bench.sh azure readings \
  --set readings.devices=10000 \
  --set readings.readings=10000
----

==== Common Configuration Options

===== Custom Docker Image

Override the default image tag to use `nightly` or a specific commit:

[source,bash]
----
./cloud/run-bench.sh azure tpch \
  --set tpch.scaleFactor=0.01 \
  --set image.tag=nightly

# Or use a specific commit SHA
./cloud/run-bench.sh azure tpch \
  --set tpch.scaleFactor=0.01 \
  --set image.tag=sha-abc1234
----

===== Skip Cleanup

By default, each run clears the cloud storage bucket and previous helm releases. To skip this cleanup:

[source,bash]
----
./cloud/run-bench.sh azure tpch \
  --set tpch.scaleFactor=0.01 \
  --no-cleanup
----

==== Auctionmark (Multi-Stage Benchmark)

Auctionmark typically runs in two stages: an initial load phase and an OLTP phase with multiple nodes/pods.

===== Load Phase

[source,bash]
----
./cloud/run-bench.sh azure auctionmark \
  --set auctionmark.scaleFactor=0.1 \
  --set auctionmark.onlyLoad=true
----

===== OLTP Phase

[source,bash]
----
./cloud/run-bench.sh azure auctionmark \
  --set auctionmark.threads=8 \
  --set auctionmark.duration=PT10M \
  --set auctionmark.noLoad=true \
  --no-cleanup
----

===== Monitoring OLTP Phase

To view logs from individual pods during the OLTP phase:

[source,bash]
----
# List all pods
kubectl get pods -n cloud-benchmark

# View logs from a specific pod (replace <pod-name> with actual pod name)
kubectl logs -f <pod-name> -n cloud-benchmark

# View logs from all pods with a specific label
kubectl logs -f -l app.kubernetes.io/name=xtdb-benchmark -n cloud-benchmark
----

=== Clear Benchmark Data Only

NOTE: Unless --no-cleanup is passed to the run-bench.sh script, this script will get called automatically at the beginning of each benchmark run.

If you need to clear benchmark data without destroying infrastructure, use the clear-bench script:

[source,bash]
----
./cloud/clear-bench.sh <aws|azure|google-cloud>
----

This will:

* Clear the Helm release
* Delete Kafka PVCs
* Empty the cloud storage bucket

=== Cleanup the Infra

To fully cleanup the storage, helm deployments and infrastructure on the cloud, you can run the "delete-infra.sh" script:

[source,bash]
----
./cloud/delete-infra.sh <aws|azure|google-cloud>
----

== Adding a new benchmark

=== Local benchmark

1. Create a new Clojure file in `src/main/clojure/xtdb/bench/` for your benchmark implementation.
2. Add a Gradle task with `createBench` in `build.gradle.kts`.
3. You can now run locally via `./gradlew <name> -P<param>=<value>`.

=== Adding to the nightly cloud run

Benchmarks run asynchronously on Azure Kubernetes.
GitHub Actions deploys the pod, waits briefly for it to start, then exits.
When the benchmark finishes, it calls back to GitHub to trigger cleanup.

Benchmarks run sequentially via a queue mechanism.
The `nightly-benchmark-suite.yml` workflow kicks off nightly at midnight UTC, dispatching the first benchmark in the queue.
When each benchmark's cleanup job completes, it dispatches the next one via `workflow_dispatch`.
The queue order is defined in the "Dispatch Next Benchmark in Queue" step of `nightly-benchmark-cleanup.yml`.
To add a new benchmark to the nightly suite, you must add it to that queue array as well as creating its own workflow file.
You can also start from a specific benchmark using the suite's `start_from` input.

The full pipeline touches several areas — benchmark code, workflows, Helm, Terraform metrics, and summary scripts.
Every piece is required for end-to-end observability (dashboards, anomaly alerts, Slack graphs).

==== 1. Benchmark code (`src/main/clojure/xtdb/bench/<name>.clj`)

* Ensure your benchmark returns a map with a `:parameters` field (e.g. `{:doc-count 100000, :scale-factor 1.0}`).
This is required for metrics filtering and dashboard queries.

==== 2. Docker image

* You'll need to build and push the `xtdb-bench` Docker image, this is triggered automatically by the run-nightly-benchmark.yml flow.
Use the "Build xtdb-bench Docker Image" GitHub Actions workflow if you've pushed your changes to `main`.

==== 3. GitHub workflows (`.github/workflows/`)

Create `nightly-benchmark-<name>.yml`::
Copy from an existing benchmark workflow (e.g. `nightly-benchmark-readings.yml`).
Set `bench_type: <name>` (must match CLI arg name).
Add `workflow_dispatch` inputs for benchmark parameters.
Set appropriate `timeout_minutes` (deploy wait) and `benchmark_timeout` (runtime limit).

Update `run-nightly-benchmark.yml`::
* Add input parameters.
* Add env vars in the deployment step.
* Add Helm `--set` conditionals.
If parameters contain commas, escape them: `${VAR//,/\\,}`.
* Add to Slack notification config.

==== 4. Helm chart (`cloud/helm/`)

Create `templates/benchmark-<name>.yaml`::
Copy from a similar benchmark template.
Update the conditional to `{{- if eq .Values.benchType "<name>" }}`.
Update metadata name, job name, and the args section to pass benchmark parameters.

Update `values.yaml`::
Add a config block with defaults for your benchmark's parameters.

==== 5. Terraform metrics (`cloud/metrics/`)

This wires up the Azure dashboard and anomaly detection alerts.

Update `variables.tf`::
Add three variables: anomaly Logic App name, alert enabled flag, and filter parameter.

Update `main.tf`::
* Add to the `benchmarks` map — name, display name, Logic App, parameter config, metric path, dashboard index.
* Add a dashboard position — 2×3 grid, each chart is 6 cols wide × 4 rows tall.

==== 6. Benchmark scripts (`cloud/scripts/src/xtdb/bench/cloud/scripts/`)

These handle log parsing, summary formatting, and Slack/GitHub output.

`charts.clj`::
Add to the `benchmark-configs` map.
The `:benchmark-name` must match the Terraform `name` exactly.

`log_parsing.clj`::
Add a `parse-log` multimethod for your benchmark type.
Parse the JSON stage lines and extract relevant stages.

`summary.clj`::
* Add a stage-row transformation helper.
* Add a summary-row builder.
* Implement three multimethod formats: `summary->table`, `summary->slack`, `summary->github-markdown`.

`tasks.clj`::
* Add validation in `summarize-log` (check for empty stages).
* Update help text to list the new benchmark type.

==== Gotchas

* *Name consistency* — the `:title` in benchmark code, `name` in Terraform, and `:benchmark-name` in `charts.clj` must all match.
* *Comma escaping* — parameters containing commas (like `"1000,100,10,1"`) need escaping in the workflow.
* *Parameter types in KQL* — string params (ISO durations) need quotes, numeric params use `todouble()`.
* *`timeout_minutes` vs `benchmark_timeout`* — workflow timeout is how long GH Actions waits for deployment; benchmark timeout is the actual runtime limit passed via `--timeout`.
These are independent.

==== Testing checklist

Once all pieces are in place, verify:

1. Manual `workflow_dispatch` triggers successfully.
2. Helm deployment starts the pod and runs the benchmark.
3. `ContainerLog` in Azure contains the benchmark's JSON log lines.
4. Dashboard chart appears after the first run.
5. Anomaly detection Logic App triggers daily.
6. Slack posts a timeseries chart and text summary on completion.
7. `terraform plan` / `terraform apply` in `cloud/metrics/` succeeds.

== CI and Grafana Dashboard Images

=== GitHub Actions

The repository includes CI workflows that can provision cloud infra, run the benchmark, and publish results.

At a high level:

* A nightly job will run benchmark workloads (e.g. TPC-H) against a permanent benchmark kubernetes cluster
* After the run, scripts export dashboard snapshots from Grafana for the time range of the run
* These snapshots (and other run artifacts) are uploaded to the workflow as GitHub Actions _artifacts_

=== How to download Grafana dashboard images from a job

1. Open GitHub → `Actions` → choose the latest benchmark run workflow
2. Scroll to the bottom to the **Artifacts** section
3. Download the artifact that contains Grafana snapshots (the name includes `grafana`)
4. Extract the archive locally to view the exported PNG images